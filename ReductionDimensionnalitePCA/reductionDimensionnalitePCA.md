# üî¨ Exp√©rimentation ‚Äì Impact des Param√®tres sur la Pr√©cision (Accuracy)

Ce document d√©crit une s√©rie d‚Äôexp√©rimentations pour analyser l‚Äôimpact de diff√©rents r√©glages (pr√©traitement des images, param√®tres PCA, configuration du mod√®le) sur la **pr√©cision d‚Äôun mod√®le PCA + R√©gression Logistique** destin√© √† d√©tecter **trois classes** : Normal, Pneumonie Bact√©rienne, Pneumonie Virale.

---

## ‚öôÔ∏è R√©glages de Base (Baseline)

| Param√®tre           | Valeur                                |
| -------------------- | ------------------------------------- |
| Mod√®le              | `LogisticRegression(max_iter=1000)` |
| R√©duction dimension | `PCA(n_components=0.95)`            |
| Taille des images    | `(400, 400)`                        |
| Format               | `Grayscale`(images aplaties)        |
| Normalisation        | Pixels entre 0 et 1                   |
| Split                | 80% train / 20% test, stratifi√©      |

**Accuracy de base :** 82%

---

## üß™ Batterie de Tests

### üîÅ 1. Variation de la taille d‚Äôimage

| ID | Modification              | Description                                    | R√©sultat (Accuracy) |
| -- | ------------------------- | ---------------------------------------------- | -------------------- |
| V1 | `image_size=(200, 200)` | Moins de pixels, traitement plus rapide        | *82%*              |
| V2 | `image_size=(128, 128)` | Test avec 128 pixel                            | 80%                  |
| V3 | `image_size=(100, 100)` | Compression agressive, possible perte d‚Äôinfos | 85%                  |

---

### üìâ 2. PCA ‚Äì R√©duction de dimension

Test Reduction PCA avec `image_size=(100, 100)`.

| ID | Modification          | Description                          | R√©sultat (Accuracy) |
| -- | --------------------- | ------------------------------------ | -------------------- |
| P0 | `n_components=0.95` | n_component base                     | 85%                  |
| P1 | `n_components=0.90` | Moins de composantes, plus rapide    | 86%                  |
| P2 | `n_components=0.99` | Pr√©serve davantage de variance      | 86%                  |
| P3 | `n_components=100`  | Nombre fixe de composantes           | 84%                  |
| P4 | `n_components=300`  | Tr√®s riche, risque de bruit inutile | 86%                  |

---

### ‚öôÔ∏è 3. Modifications du mod√®le de r√©gression logistique

| ID | Modification      | Description                                                                                | R√©sultat (Accuracy) |
| -- | ----------------- | ------------------------------------------------------------------------------------------ | -------------------- |
| M0 | `max_iter=1000` |                                                                                            | 86%                  |
| M1 | `max_iter=2000` | Plus d‚Äôit√©rations pour convergence                                                       | 84%                  |
| M2 | `solver='saga'` | Optimis√© pour les grands jeux de donn√©es                                                 | 85%                  |
| M3 | `penalty='l1'`  | Lasso : favorise des poids nuls (sparse model)<br />`max_iter=1000` +¬†`solver='saga'` | *86%*              |
| M4 | `C=0.1`         | R√©gularisation forte                                                                      | 78%                  |
| M5 | `C=10.0`        | Faible r√©gularisation, plus de flexibilit√©                                               | *84 %*             |

---

## üìä R√©sum√© des Exp√©riences

| Test ID | Accuracy | Observations                                                    |
| ------- | -------- | --------------------------------------------------------------- |
| V1      | 86%      | R√©duction de taille ‚Üí plus rapide, attention √† la pr√©cision |
| P2      | 86%      | PCA plus riche ‚Üí utile si perte d‚Äôinfo                        |
| M3      | 86%      | Mod√®le plus sparse, mais perte possible de performance         |

---

## üõ†Ô∏è Code type pour une exp√©rimentation

```python
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('pca', PCA(n_components=0.99)),
    ('clf', LogisticRegression(
        solver='saga',
        penalty='l1',
        C=0.5,
        max_iter=2000
    ))
])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
```
